<!DOCTYPE html>
<html>
<head>
  <!-- (Meta tags and other head elements remain unchanged) -->
  <title> ðŸŽ¥ðŸ”§ðŸ”—Visual Editing with LLM-based Tool Chaining: An Efficient Distillation Approach for Real-Time Applications </title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
  <!-- Hero section -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ðŸŽ¥ðŸ”§ðŸ”—Visual Editing with LLM-based Tool Chaining: An Efficient Distillation Approach for Real-Time Applications </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/oren-sultan-93039146/" target="_blank">Oren Sultan<sup>1, 2</sup></a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/alexander-khasin-63871652/" target="_blank">Alex Khasin<sup>2</sup></a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/guy-shiran-b66650127/" target="_blank">Guy Shiran<sup>2</sup></a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/asi-messica-02a80/" target="_blank">Asnat Greenstein-Messica<sup>2</sup></a>,</span>
              <span class="author-block">
                <a href="http://www.hyadatalab.com/" target="_blank">Dafna Shahaf<sup>1</sup></a></span>
              </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                (1) The Hebrew University of Jerusalem; (2) Lightricks<br>
              </span>
              <br>
              <span class="author-block">
                EMNLP 2024 (Main Conference, Industry Track)
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://aclanthology.org/2022.emnlp-main.232.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/orensultan/AIRecolor" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://arxiv.org/abs/2210.12197" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We present a practical distillation approach to fine-tune LLMs for invoking tools in real-time applications. We focus on visual editing tasks; specifically, we modify images and videos by interpreting user stylistic requests, specified in natural language (``golden hour''), using an LLM to select the appropriate tools and their parameters to achieve the desired visual effect. We found that proprietary LLMs such as GPT-3.5-Turbo show potential in this task, but their high cost and latency make them unsuitable for real-time applications. In our approach, we fine-tune a (smaller) student LLM with guidance from a (larger) teacher LLM and behavioral signals. We introduce offline metrics to evaluate student LLMs. Both online and offline experiments show that our student models manage to match the performance of our teacher model (GPT-3.5-Turbo), significantly reducing costs and latency. Lastly, we show that fine-tuning was improved by 25% in low-data regimes using augmentation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->



  <!-- Image Section -->
  <section class="image-section" style="margin-top: 100px;">
    <div class="container is-max-desktop" style="display: flex; flex-direction: column; justify-content: center; align-items: center; height: 30vh; text-align: center; padding: 20px;">
      <img src="static/images/website_img1.jpg" alt="Website" style="max-width: 80%; height: auto; margin-bottom: 15px;">
      <h2 class="subtitle">
        An illustration of our visual editing task. Users input an image/video and specify the desired visual appearance (upper row: source images, middle: user intents). An LLM interprets these intents, selects tools, and sets parameters. The bottom row displays the generated images by applying the LLM's output in our app.  For example, inputting ``Morocco'' (left) results in warm hues typical of Moroccan landscapes, reflecting its deserts.
      </h2>
    </div>
  </section>

  <!-- Teaser video -->
  <section class="hero teaser" style="margin-top: 100px;">
    <div class="container is-max-desktop" style="display: flex; justify-content: center; align-items: center; height: 100vh; flex-direction: column; text-align: center;">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop style="width: 40%; height: 400px;">
          <!-- Your video here -->
          <source src="static/videos/ai_recolor_3examples.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle">
          A+B: Example input â€“ two analogous texts, describing the animal cell (base) and factory (target). <br/>
          C: Our algorithmâ€™s output. The nodes are entities (clusters of text spans). Edge width represents similarity between entities in terms of the roles they play in the text. For example, the boxes on the left illustrate the similar roles associated with the red and the pink entities. Showing the mapping along with its justification (the similar roles) renders our output easy to interpret.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/website_img1.jpg" alt="MY ALT TEXT"/>
            <h2 class="subtitle has-text-centered">
              A+B: Example input â€“ two analogous texts, describing the animal cell (base) and factory (target). <br/> C: Our algorithmâ€™s output. The nodes are entities (clusters of text spans). Edge width represents similarity between entities in terms of the roles they play in the text. For example, the boxes on the left illustrate the similar roles associated with the red and the pink entities. All the mappings our algorithm found are correct, but two are missing (ribosomes/machines and endoplasmic reticulum/hallways). Showing the mapping along with its justification (the similar roles) renders our output easy to interpret.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/website_img2.jpg" alt="MY ALT TEXT"/>
            <h2 class="subtitle has-text-centered">
              Examples of analogies mined by our method (FMQ). <br/>
              While SBERT was able to find almost only paragraphs on the same topic (self-analogies), <br/> our method has found many close and far analogies which are more abstract.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/website_img3.jpg" alt="MY ALT TEXT"/>
            <h2 class="subtitle has-text-centered">
              Our method identifies the correct mappings 87% of the time for procedural texts from ProPara dataset. <br/>
              Here is an example of a far analogy between how hurricanes form and what causes a volcano to erupt.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/website_img4.jpg" alt="MY ALT TEXT"/>
            <h2 class="subtitle has-text-centered">
              Although not designed for them, our method achieves 94% precision on cognitive-psychology stories. <br/>
              Here is a famous example of an analogy between the stories of a general and a surgeon.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->

  <!-- Youtube video -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">EMNLP 2022 Teaser Video</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/XWM7MM4M2Ws" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End youtube video -->

  <!-- Paper poster -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">EMNLP 2022 Poster</h2>
        <iframe src="static/pdfs/EMNLPFinalPosterCorrectSize.pdf" width="100%" height="550"></iframe>
      </div>
    </div>
  </section>
  <!-- End paper poster -->

  <!-- BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{sultan2022life,
  title={Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes},
  author={Sultan, Oren and Shahaf, Dafna},
  journal={arXiv preprint arXiv:2210.12197},
  year={2022}
}</code></pre>
    </div>
  </section>
  <!-- End BibTeX citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
              <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->
  <script type="text/javascript">
  var sc_project=12822541;
  var sc_invisible=1;
  var sc_security="46936e0e";
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/12822541/0/46936e0e/1/" alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->

</body>
</html>
